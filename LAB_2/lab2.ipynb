{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MALIS Lab Session 2 - Fall 2019</h1>\n",
    "November 12, 2019\n",
    "\n",
    "___\n",
    "\n",
    "Martin Guyard\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this lab is to practice with Neural Networks (Multi-Layer Perceptrons) via simple classification experiments and the (partial) implementation of the feedforward and backpropagation procedures. For this lab, the implementation of the MLP simulator is in Python 3.\n",
    "\n",
    "#### Learning goals\n",
    "After this lab, you should be able to:\n",
    "1. Be familiar with the elements required to define the architecture of a neural network (NN).\n",
    "2. Understand the two procedures needed to train a neural network: feedforward and backpropagation\n",
    "3. Understand the role of the learning rate and the number of iterations in the training process of a NN and how it these can affect performance.\n",
    "\n",
    "#### Instructions:\n",
    "Experiments should be made by groups of two students. Each group should produce a Jupyter Notebook with all their results and comments. We strongly encourage the addition of plots and visual representation to the report, bearing in mind that comments on the graphical data are still necessary. Code for adding images to your notebook: ```<img src=\"path/to/image.png\" />```. <Plateforme soumission des notebooks>\n",
    "\n",
    "**Submission deadline**: Nov. 18 23.59 (submision via Moodle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>\n",
    "There are three parts to this lab session. \n",
    "\n",
    "1. A \"theoretical\" part: Given a set of training examples you have to decide on the architecture of the feed-forward neural network such as; number of layers, number of neuron per layers and finally the values of the weights. \n",
    "\n",
    "2. A \"programming\" part: Given the skeleton of the Python code of an MLP simulator, implement the missing functions (feedforward and backpropagation procedures). \n",
    "\n",
    "3. An \"experimental\" part: Having completed the implementation of the MLP simulator, the final step consist on training the network and testing it.\n",
    "\n",
    "<h2>Part 1: Design a neural network</h2>\n",
    "The aim of this part is to get a better understanding of the basics of Neural Networks construction. A number of sample points on a 128 by 128 grid have been assigned one out of three colors (red, green or blue). You should build a Neural Network with two inputs and three outputs which provides the exact coloring for these points. The problem can be visualized in the following figure: \n",
    "\n",
    "<img src=\"data_set.jpg\" />\n",
    "\n",
    "The file set30.x1x2rgb (in .\\data\\) contains the data corresponding to the problem defined above. The visual representation of the problem (above figure) is stored in data_set.jpg.\n",
    "\n",
    "The problem:\n",
    "\n",
    "Pairs of x1 and x2 coordinates (both ranging between 0 and 127) are associated with a specific color: \n",
    "\n",
    "* Red: output 1 0 0, \n",
    "* Green: output 0 1 0, \n",
    "* Blue: output 0 0 1. \n",
    "\n",
    "The objective of the network is to correctly determine for any given (x1, x2) coordinate pair the corresponding color. \n",
    "Your task is to <b>manually define a Neural Network which performs this task perfectly</b>. There is no need for programming or iterative training. The transfer function is assumed to be the step function: \n",
    "\n",
    "$f(t) = (t > 0)$ (it is equal to 1 if t is positive, 0 otherwise). \n",
    "\n",
    "Of course, it is your task to define the number of layers, the number of neurons per layer, and the exact values for the weights. \n",
    "\n",
    "<i>Hint: We may remember the XOR problem and how it was solved.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Your answer:</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters are **linearly separable by 2 hyperplans.** As we did with the xor function, a simple MLP with 2 hidden units and 3 output units is enough to separate the data.\n",
    "<img src=\"data_sep.png\" />`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hidden units corresponds to a linear relation that separate data. First hidden unit separate RED / (BLUE, GREEN), second hidden unit separate BLUE/GREEN\n",
    "\n",
    "\n",
    "<br>\n",
    "$h(a)=step(a)$ : the transfert function is the step function.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**1. Separate RED/(BLUE, GREEN): First hidden unit**\n",
    "<br>(0, 60) and (120, 0)\n",
    "<br>$0*w_{11}+80*w_{21} + b_1 = 0$\n",
    "<br>\n",
    "$120*w_{11}+0*w_{21} + b_1 = 0$\n",
    "<br> Thus choosing $b_1=-1$, $w_{11}=1/120$, $w_{21}=1/80$ \n",
    "<br>\n",
    "If (x1, x2) is **in** the **RED** cluster, then $h(w_{11}x_1+w_{21}x_2-b_1)=0$\n",
    "<br>\n",
    "If (x1, x2) is **NOT in** the **RED** cluster, then $h(w_{11}x_1+w_{21}x_2-b_1)=1$\n",
    "\n",
    "\n",
    "**2. Separate BLUE/GREEN: Second hidden unit**\n",
    "<br>(60, 60) and (100, 120)\n",
    "<br>$60*w_{12}+60*w_{22} + b_2 = 0$\n",
    "<br>\n",
    "$100*w_{12}+120*w_{22} + b_2 = 0$\n",
    "<br> Thus choosing $b_2=-1$, $w_{12}=-1/30$, $w_{22}=1/20$\n",
    "<br>\n",
    "If (x1, x2) is **in** the **GREEN** cluster, then \n",
    "$h(w_{12}x_1+w_{22}x_2-b_2)=1$ and $h(w_{11}x_1+w_{21}x_2-b_1)=1$\n",
    "<br>\n",
    "If (x1, x2) is **in** the **BLUE** cluster, then \n",
    "$h(w_{12}x_1+w_{22}x_2-b_2)=0$ and $h(w_{11}x_1+w_{21}x_2-b_1)=1$\n",
    "\n",
    "**3. Find weights of the output layer**\n",
    "<br>\n",
    "to find the output units weights we can draw truth table and find the right combination of weight and biases\n",
    "<br>\n",
    "**example:**\n",
    "RED truth table:\n",
    "\n",
    "|  H1 | H2  |  Z |\n",
    "|---|---|---|\n",
    "|   0|   0|   1|\n",
    "|   0|   1|   1| \n",
    "|   1|   0|   0|\n",
    "|   1|   1|   0|\n",
    "\n",
    "This works with w1 = -1, w2 = 0 and b = +0.5\n",
    "\n",
    "<br>\n",
    "\n",
    "**OUR MLP**\n",
    "\n",
    "<img src=\"MLP.png\" />`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 1 hidden layers with 2 neurons \n",
    "\n",
    "Input layer:  2 units,  x1   x2\n",
    "\n",
    "First hidden layer:\n",
    "    2 neurons:\n",
    "    neuron 1:     w11 = +1/120\n",
    "                  w21 = +1/80\n",
    "                  b1  = -1\n",
    "                  \n",
    "    neuron 2:     w12 = -1/30\n",
    "                  w22 = +1/20\n",
    "                  b2  = -1\n",
    "\n",
    "output layer:\n",
    "    3 neurons:\n",
    "    neuron 1:     w11 = -1\n",
    "                  w21 = 0     \n",
    "                  b1  = +0.5\n",
    "                  \n",
    "    neuron 2:     w12 = +1\n",
    "                  w22 = -1   \n",
    "                  b2  = -0.5\n",
    "                  \n",
    "    neuron 3:     w13 = +1\n",
    "                  w23 = +1\n",
    "                  b3  = -1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 2: Implementation of the MLP simulator</h2>\n",
    "The task here is to implement the missing parts of a code written to simulate multi-layer perceptrons. The code can be found in your directory under the filename utils.py (but you will not edit that file, all your code will be written in your notebook). Here is a brief explanation about the MLP simulator: \n",
    "\n",
    "A network description file has to be provided. This is a text file which contains information about the number of layers in the network and the number of units (neuron) for each layer. Here is an example of such a file: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2\n",
    "3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example describes a 2 layer network with 2 hidden units and 3 output units. \n",
    "Additionally a pattern (or example set) file has to be provided. This file contains a number of example pattern with input and output values. For an example of such a file look at ./data/set30.x1x2rgb.\n",
    "\n",
    "As you know, activation functions of an MLP need to be differentiable to train it. Therefore, we replace the step function by a sigmoid function.\n",
    "\n",
    "Now that you have a broad overview of the program your task is to <b>implement the feedforward function of the Neuron class</b>. Obviously, you can find help in the notes from the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell to import relevant classes and functions\n",
    "from utils import Neuron, Dataset, Layer, MLP, sigmoid, d_sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Your answer:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(self):\n",
    "    res = 0.\n",
    "    for i in range(len(self.inputs)):\n",
    "        #      x1*w1+...+xn*wn\n",
    "        res += self.inputs[i]*self.weights[i]\n",
    "    res += self.bias\n",
    "    self.u = res\n",
    "    self.out = sigmoid(res)\n",
    "\n",
    "Neuron.feedforward = feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the Backpropagation function, <b>write the recursive formula for the partial derivative of the error with respect to the activation (neuron j of layer i) as a function of the weights and partial derivative of the error in layer i+1 from the course material</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Your answer:</h3>\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial a^{(i)}_j} = h'(a^{(i)}_j)\\cdot\\sum_{k=1}^{\\ n} \\frac{\\partial L}{\\partial a^{(i+1)}_k}\\cdot\\ w_{kj}$$\n",
    "\n",
    "Using the chain rules.\n",
    "n being the number of neurons on the layer i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, <b>implement the compute_gradients() and the apply_gradient() functions of the MLP class</b>.\n",
    "\n",
    "<h3>Your answer:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(self):\n",
    "    # First compute derivatives for the last layer\n",
    "    layer = self.layers[-1]\n",
    "    for i in range(len(layer)):\n",
    "        # Compute dL/du_i\n",
    "        neuron = layer.neurons[i]\n",
    "        o = neuron.out # o = sigmoid(u)\n",
    "        u = neuron.u\n",
    "        t = self.gt[i]\n",
    "        \n",
    "        neuron.d_u = 2*d_sigmoid(u)*(o-t)\n",
    "        \n",
    "        for j in range(len(neuron.weights)):\n",
    "            # Compute dL/dw_ji\n",
    "            # using chains derivation: dL/dw_ji = dL/du_i * du_i/dw_ji\n",
    "            neuron.d_weights[j] = neuron.d_u * neuron.inputs[j]\n",
    "\n",
    "    # Then compute derivatives for other layers\n",
    "    for l in range(2, len(self.layers)):\n",
    "        layer = self.layers[-l]\n",
    "        next_layer = self.layers[-l+1]\n",
    "        for i in range(len(layer)):\n",
    "            # Compute dL/du_i\n",
    "            neuron = layer.neurons[i]\n",
    "            d_u = 0.\n",
    "            u = neuron.u\n",
    "            for j in range(len(next_layer)):\n",
    "                next_neuron = next_layer.neurons[j]\n",
    "                d_u += d_sigmoid(u) * next_neuron.d_u * next_neuron.weights[i]\n",
    "                \n",
    "            neuron.d_u = d_u\n",
    "            for j in range(len(neuron.weights)):\n",
    "                # Compute dL/dw_ji\n",
    "                # using chains derivation: dL/dw_ji = dL/du_i * du_i/dw_ji\n",
    "                neuron.d_weights[j] = neuron.d_u * neuron.inputs[j]\n",
    "                \n",
    "                \n",
    "\n",
    "def apply_gradients(self, learning_rate):\n",
    "    # Change weights according to computed gradients\n",
    "    for i in range(1, len(self.layers)):\n",
    "        layer = self.layers[i]\n",
    "        for j in range(1, len(layer)):\n",
    "            neuron = layer.neurons[j]\n",
    "            for k in range(len(neuron.d_weights)):\n",
    "                neuron.weights[k] -= learning_rate * neuron.d_weights[k]\n",
    "                \n",
    "            neuron.bias -= learning_rate * neuron.d_u\n",
    "\n",
    "MLP.compute_gradients = compute_gradients\n",
    "MLP.apply_gradients = apply_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Part 3: Training and Recall experiments</h2>\n",
    "\n",
    "Train the network on the problem stated in Part 1, using the training set set120.x1x2rgb and the following parameters:\n",
    "* learning rate: 2.0; \n",
    "* number of training cycles: 1000\n",
    "\n",
    "In order to do so you will need to create a network definition file (as described in the introduction) containing the details of the network architecture. \n",
    "Evaluate the accuracy using set30.x1x2rgb as the test set (you can use the setdataset() function of the MLP class to change between training and test sets).\n",
    "\n",
    "Experiment with the learning rate and the number of training cycles. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Your answer:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Result on test data ===\n",
      "Accuracy: 96.66666666666667 %\n"
     ]
    }
   ],
   "source": [
    "train_datafile = \"data/set120.x1x2rgb\"\n",
    "train_data = Dataset(train_datafile)\n",
    "\n",
    "test_datafile = \"data/set30.x1x2rgb\"\n",
    "test_data = Dataset(test_datafile)\n",
    "\n",
    "nnfile = \"data/NN.dat\"\n",
    "mlp = MLP(nnfile, train_data, print_step=25, verbose=False)\n",
    "\n",
    "mlp.train(2000, 0.5)\n",
    "mlp.make_plot()\n",
    "\n",
    "print(\"=== Result on test data ===\")\n",
    "mlp.setdataset(test_data)\n",
    "mlp.print_accuracy()\n",
    "\n",
    "# .cmp_acc([list of points [], [] ...])\n",
    "# is a method to test directly some sample\n",
    "# eg. mlp.cmp_acc([[120,60]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Your comments</h3>\n",
    "\n",
    "___\n",
    "\n",
    "Each epoch corresponds to a backpropagation of the gradient on the entire network. This means that each epoch is slightly changing (according to the learning rate) the weights and bias of the neurons.\n",
    "\n",
    "\n",
    "**On one hand**, if the learning is to big (in this case > 1.5) the accuracy over the epoch is very unstable. A learning rate (aka step size) too big might cause gradient divergence. The SGD or GD algorithm converge only with some assumptions on the convexity of the funtion to optimize and the step size.\n",
    "\n",
    "\n",
    "**On the other hand**, it the step size is too small the network needs a lot of epoch to give a good enough accuracy. Because at each epoch the weights and bias are changed according to the learning rate if it is too small the network will require a lot of backpropagation stage (ie epoch) to change substatially the network.\n",
    "\n",
    "\n",
    "So there is the following tradeoff:\n",
    "- small learning rate might converge and give better better accuracy BUT require lots of epochs = lots of computation\n",
    "- big learning rate might not converge and gave poorer accuracy BUT require less epoch = less computation\n",
    "\n",
    "\n",
    "\n",
    "So in the end choosing those meta-parameters for a simple neural network is not trivial, the right tradeoff is too choose a balance between targeted accuracy (ie accuracy good enough for the problem we are trying to solve) and computation time (number of epoch to reach this level of accuracy).\n",
    "\n",
    "\n",
    "My heuristic approach to choose these meta-parameters would be the following:\n",
    "- 1) Train with epoch=1000 and lrate=2.0\n",
    "- 2) Unstable accuracy so train again with decreasing lrate=0.5 and epoch=1000\n",
    "- 3) Stable increasing accuracy over the epoch but increase too slowly, so train again with increasing epoch=2000 and lrate=0.5\n",
    "- 4) Achieve stability and good enough accuracy\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
